\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{eso-pic}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{placeins}


% Include other packages here, before hyperref.
\usepackage{my_macros}
\usepackage{paralist}
\usepackage{amsthm}
\usepackage{dirtytalk}
%\usepackage{framed}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{setspace}
%for tikz
\usepackage{tikz,pgfplots}
\usepackage{pgfplotstable}
\usepackage{filecontents}
%\usepackage{scrextend}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\footlabel}[2]{%
    \addtocounter{footnote}{1}%
    \footnotetext[\thefootnote]{%
        \addtocounter{footnote}{-1}%
        \refstepcounter{footnote}\label{#1}%
        #2%
    }%
    $^{\ref{#1}}$%
}

\newcommand{\footref}[1]{%
    $^{\ref{#1}}$%
}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usetikzlibrary{arrows,petri,topaths}
\usetikzlibrary{positioning,fit,calc}
\usetikzlibrary{shapes.arrows,chains,decorations.pathreplacing,fadings}
\usetikzlibrary{calc, matrix, backgrounds}

\pgfplotsset{every axis/.append style={
  every axis y label/.style = {at={(ticklabel cs:0.5)}, rotate=90, anchor=south},
  axis x line = {bottom},
  axis y line = {left},
  tick align = outside,
  ymajorgrids = true,
%  legend style = {draw=none, at={(1.05, 0.5)}, anchor=west, font=\small},
  legend style = {font=\tiny},
  legend columns = 1,
  every axis plot/.append style = {line width=1pt},
  label style = {font=\small},
  tick label style={font=\small},
  scaled ticks = false,
}}

% Two Colored Circle Split 
\makeatletter
\tikzset{circle split part fill/.style  args={#1,#2}{%
 alias=tmp@name, 
  postaction={%
    insert path={
     \pgfextra{% 
     \pgfpointdiff{\pgfpointanchor{\pgf@node@name}{center}}%
                  {\pgfpointanchor{\pgf@node@name}{east}}%            
     \pgfmathsetmacro\insiderad{\pgf@x}
      \fill[#1] (\pgf@node@name.base) ([xshift=-\pgflinewidth]\pgf@node@name.east) arc
                          (0:180:\insiderad-\pgflinewidth)--cycle;
      \fill[#2] (\pgf@node@name.base) ([xshift=\pgflinewidth]\pgf@node@name.west)  arc
                           (180:360:\insiderad-\pgflinewidth)--cycle;            
         }}}}}  
 \makeatother  

%\usepackage{tkz-berge}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\definecolor{shadecolor}{rgb}{0.01,0.199,0.1}
\usepackage{xargs} 
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage{bm}% Ã¤ndert \boldsymbol
\def\arraystretch{0.8}
\renewcommand{\tabcolsep}{2pt}
\newcommand{\thickline}{2pt}
\newcommand{\scatterplotpath}{./scatterplots/}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\newcommand{\OR}{\textrm{ or }}

% \cvprfinalcopy % *** Uncomment this line for the final submission


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\realplot}[1]{
\begin{tikzpicture}
    \begin{axis}
        \addlegendimage{empty legend}\addlegendentry{Matrix #1}
        \addplot {0};
        \addplot {1};
        \addplot {2};
        \addplot {3};
    \end{axis}
\end{tikzpicture}}  

% \newcommand{\anytimeplot}[1]{
%   \addplot[color=brown,mark=x] table[x=time,y=HC]{data3/knott-3d-300.data};              \addlegendentry{HC}
%   \addplot[color=black,mark=x] table[x=time,y=HC-CGC]{data3/knott-3d-300.data};          \addlegendentry{HC-CGC}
%   \addplot[color=red,mark=x] table[x=time,y=CGC]{data3/knott-3d-300.data};               \addlegendentry{CGC}
%   \addplot[color=gray,mark=o] table[x=time, y=ogm-KL]{data3/knott-3d-300.data};          \addlegendentry{KL}
%   \addplot[color=purple,mark=x] table[x=time, y=MCR-CCFDB]{data3/knott-3d-300.data};     \addlegendentry{MC-R}
%   \addplot[color=blue,mark=x] table[x=time, y=MCI-CCIFD]{data3/knott-3d-300.data};       \addlegendentry{MC-I}
%   \addplot[color=green,mark=x] table[x=time, y=DYNCC-HC-MC]{data3/knott-3d-300.data};    \addlegendentry{Fusion-HC-MC}
%   \addplot[color=cyan,mark=x] table[x=time, y=DYNCC-HC-CGC]{data3/knott-3d-300.data};    \addlegendentry{Fusion-HC-CGC}
%   \addplot[color=orange,mark=x] table[x=time, y=DYNCC-WS-MC]{data3/knott-3d-300.data};  \addlegendentry{Fusion-WS-MC}
%   \addplot[color=orange,mark=x] table[x=time, y=DYNCC-WS-CGC]{data3/knott-3d-300.data};  \addlegendentry{Fusion-WS-CGC}
% }
\newcommand{\anytimeplot}[1]{
  \addplot[color=brown,mark=x] table[x=time,y=HC]{data3/#1.data};              \addlegendentry{HC}
  \addplot[color=black,mark=square] table[x=time,y=HC-CGC]{data3/#1.data};          \addlegendentry{HC-CGC}
  \addplot[color=red,mark=x] table[x=time,y=CGC]{data3/#1.data};               \addlegendentry{CGC}
  \addplot[color=gray,mark=o] table[x=time, y=ogm-KL]{data3/#1.data};          \addlegendentry{KL} 
  \addplot[color=yellow!50!black,mark=square] table[x=time, y=ogm-mcfusion-HC-CF*]{data3/#1.data};  \addlegendentry{Fusion}
  \addplot[color=purple,mark=o] table[x=time, y=MCR-CCFDB]{data3/#1.data};     \addlegendentry{MC-R}
  \addplot[color=blue,mark=o] table[x=time, y=MCI-CCIFD]{data3/#1.data};       \addlegendentry{MC-I}
  \addplot[color=green,mark=o] table[x=time, y=DYNCC-HC-MC]{data3/#1.data};    \addlegendentry{Fusion-HC-MC}
  \addplot[color=cyan,mark=x] table[x=time, y=DYNCC-HC-CGC]{data3/#1.data};    \addlegendentry{Fusion-HC-CGC}
  \addplot[color=orange,mark=o] table[x=time, y=DYNCC-WS-MC]{data3/#1.data};  \addlegendentry{Fusion-WS-MC}
  \addplot[color=pink,mark=x] table[x=time, y=DYNCC-WS-CGC]{data3/#1.data};  \addlegendentry{Fusion-WS-CGC}
}
%  \addplot[color=yellow!50!black,mark=o] table[x=time, y=ogm-mcfusion-HC-CF*]{data3/#1.data};  \addlegendentry{Fusion}
  %\addplot[color=green!50,mark=o] table[x=time, y=PIVOT*]{data3/#1.data};          \addlegendentry{PIVOT-BOEM}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}
%%%%%%%%% TITLE
\include{my_macros.tex}
%\title{Correlation Clustering with Dynamic Super-Nodes (DySNCC)}
\title{Fusion Moves for Correlation Clustering}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}


%%%%%%%%% ABSTRACT
\begin{abstract}
  \color{red} \textbf{TODO:}
   We address the problem of partitioning a  graph
   into a previously unknown number of clusters.
   Among all partitions, the one with the minimal 
   sum of of cut edge weights is chosen. 
   This problem is known as correlation clustering 
   or multicut.
   We propose a general framework to find
   high quality approximate solutions for 
   this NP-hard problem based on a move making algorithm:
   We use candidate solutions from a proposal generator
   to iteratively improve the best observed solution similar
   to fusion moves.
   The proposed solver outperforms any other solver
   w.r.t. to any time performance.
   The solutions found by this solver are close
   to global optimal solutions w.r.t. energy
   and problem specific measurements as VI for
   image segmentation problems.

\end{abstract}
\section{Introduction}

Correlation clustering~\cite{Bansal-2002} also known as the multicut problem~\cite{chopra_1993_mp} 
is a basic primitive in computer vision~\cite{andres_2011_iccv,kroeger_2012_eccv,yarkony_2012_eccv,alush_2013_simbad} and data mining~\cite{Chierichetti-2014,Arasu-2009,Sadikov-2010} as a first step
towards understanding an image or data in general. 
The correlation clustering problem is the following:
Given an undirected edge weighted graph, 
where positive weights (\emph{attractive}) encourage the adjacent nodes to be in the same cluster
and negative weights (\emph{repulsive}) encourage the nodes to stay in different cluster, 
find the clustering of nodes such that the sum over weights of edges between clusters is minimized.
Note that this problem formulation does not included a predefined number of clusters.

Despite the clear mathematical formulation and nice properties of correlation clustering
to perform a one-shot agglomeration clustering,
the main draw back of correlation clustering is scalability~\cite{nunez_iglesias_2013}
which arise from the NP-hardness of the problem~\cite{Bansal-2002}.

In signed social networks, where positive and negative edges encodes friend and foe relationships, respectively,
correlation clustering is a natural way to detect communities~\cite{Chierichetti-2014,Chen-2012}.
Correlation clustering can also been used to cluster query refinements in web search~\cite{Sadikov-2010}.
Because social and web-related networks are often very huge, heuristic methods, \eg the PIVOT-algorithm~\cite{Ailon-2008},
are very popular~\cite{Chierichetti-2014}.

In computer vision applications, unsupervised image segmentation algorithms usually start with an over-segmentation
into superpixels (superregions), which are then clustered into ``perceptually meaningful''
regions by correlation clustering on sparse graphs.
Such an approach has been shown to yield
state-of-the-art results on the Berkeley Segmentation Database
\cite{andres_2011_iccv,yarkony_2012_eccv,alush_2013_simbad}.

Recently, Beier~\etal~\cite{beier_2014_cvpr} presented a promising 
approximative method called Cut, Clue \& Cut.
While this scales well for planar graphs, it does not for non planar graphs and has a bad any-time performance.
A further inherent drawback of this method is that the subproblems are not limited in their size.
%
Consequently, partition problems on large scale data, \eg
huge volume images in computational neuroscience~\cite{kroeger_2012_eccv}
or social networks~\cite{Leskovec-2010}, 
are not tractable, because the runtime explodes or the 
approximations are far away from being useful.

In the present work we will suggest some alternative approaches to deal with large scale correlation clustering problems.
In the end we ill present a method that can be combined with any other correlation clustering method and 
allows to reduce the overall problem to a sequence of correlation clustering problems which are by magnitudes smaller.


\textbf{Contribution:}
In the present work we present some novel approaches that are designed for large scale correlation clustering problems.
First, we define a novel energy based agglomerative clustering algorithm that monotonically increase the energy.
With this at hand we show how to improve the anytime performance of Cut, Clue \& Cut.
Second, we introduce cluster-fusion moves, which extend the original fusion moves~\cite{Lempitsky-2010} 
used in supervised segmentation to the unsupervised case and give a polyhedral interpretation of this algorithm.
We propose two versatile proposal generators, and evaluate the proposed methods on existing and new benchmark problems.
That experiments show that we can improve the computation time by one to two magnitudes without worsening the segmentation 
quality significantly.


% Given an edge weighted graph, positive weights (\emph{attractive})
% encourage the adjacent nodes to be in the same 
% connected component, while negative weights (\emph{repulsive}) encourage
% the nodes to stay in different connect components.
% Therefore the sign of the weights encodes if two
% nodes should be merged or not and the magnitude of the weights encodes
% the certainty of this desire.
% The objective of correlation clustering / multicuts
% is finding the cut with a minimal sum of cut edge weights.
% The number of connected components / clusters is discovered
% from the weights ( rewrite?!?).
% Correlation clustering / the multicut is NP-hard \cite{???}.
% %
% Despite the NP-hardness, correlation clustering has been 
% successfully used for
% \begin{inparaenum}[(i)]
%     \item partitioning a superpixel region adjacency graph~\cite{andres_2011_iccv,kroeger_2012_eccv}
%     \item with optional long range repulsive edges~\cite{andres_2013_emmcvpr}.
%     \item Alush and Globerger showed how to average multiple segmentations with the multicut objective~\cite{alush_2012_pami}.
%     \item Multicuts can also be used for interactive segmentation~\cite{bagon_2011_arxiv},
%     \item for co-segmentation~\cite{glassner_2011_cvpr}
%     \item and to cluster sparse and graphs~\cite{???}.
% \end{inparaenum}


% Despite the nice properties of correlation clustering
% to perform a \say{one-shot agglomeration of supervoxels} ~\cite{nunez_iglesias_2013},
% the main draw back of correlation clustering is scalability~\cite{nunez_iglesias_2013}.
% Existing solvers for correlation clustering do not scale for non planar graphs.
% Even state of the art approximate move making algorithm fail 
% to give good approximations non planar graphs.
% When increasing the problem sizes, either the runtime explodes or the 
% approximations are far away from being useful.


% \textbf{Contribution:}
% \begin{inparaenum}[(i)]
% \item Within this work we propose a fast and scalable move making algorithm for correlation clustering,
% \item which generalized  fusion moves \cite{???} to correlation clustering problems.
% \item We reduce the inference to a series of moves where
% each moves optimizes over a subspace spanned by the current best solution
% and a proposal solution.
% \item We propose two versatile proposal generators,
% \item and show how to optimize these moves.
% \item We give a polyhedral interpretation of this algorithm,
% \item and evaluate the proposed method 
% on existing benchmark problems.
% \item We show state of the art any time performance on those instances.
% \end{inparaenum}[(i)]

\textbf{Outline:}
In sec.~\ref{sec:problem_formulation} we will give a 
detailed problem definition where we introduce 
the correlation clustering objective.
In sec.~\ref{sec:related_work} we will 
discuss existing methods for correlation 
clustering and briefly explain the concept of fusion moves.
In sec: ~\ref{sec:cc_fm} we describe our proposed
method and show the properties of the algorithm.
In sec.~\ref{sec:exp} we show an evaluation
of the method on existing parameter  and discuss the effects of parameters.
Future work will be discussed in sec. \ref{sec:future} and
we will conclude in ~\ref{sec:conclusion}.
%-------------------------------------------------------------------------

\section{Notation and Problem Formulation}\label{sec:problem_formulation}
Let $G=(V,E, w)$ be a weighted graph of nodes $V$ and edges $E$.
%
The function $w : E \rightarrow \mathbb{R}$ assigns a weight to each edge.
A positive weight expresses the desire that two adjacent nodes should
be merged, whereas a negative weight indicates
that these nodes should be separated into two different regions.
%
A \emph{subgraph} $G_A = \{A, E_A, w\}$ consists
of nodes $A \subseteq V$ and edges $E_A := E\cap (A\times A)$.
%
A segmentation of the graph $G$ can by either given by an 
node labeling $l \in \mathbb{N}^{|V|}$
or an edge labeling $y \in\{0,1\}^{|E|}$.  
An edge labeling is only consistent if it contains no dangling edges~\cite{kappes_2013_arxiv}.
We denote the set of all consistent edge labelings by $P(G)\subset\{0,1\}^{|E|}$.
The convex hull of this set is known as the \emph{multicut polytope} $MC(G) = \textrm{conv}(P(G))$.


%$\rho_y : P(G) \to P(G_y)$
%$\rho_y^{-1} : P(G_y) \to P(G)$


% \begin{itemize}
%   \item Multicut in computer vision, applications, sparse segmentation -> IMPORTANT PROBLEM
%   \item Problem: Existing Methods does not scale, even CGC. - Large scale 3d problems
%   \item Contribution: Fast scalable method using novel fusion moves for correlation clustering
% \end{itemize}

% Def: Graph, weighted graph, cut, multicut 

Given a weighted graph $G=(V,E,w)$ we consider the problem of segmenting $G$ such that the costs
of the edges between distinct segments is minimized. This can be formulated in the node domain
by assigning each node $v$ a label $l_v \in \mathbb{N}$
\begin{align}
  l^* &= \argmin_{L \in \mathbb{N}^{|V|}} \sum_{ (i,j) \in E } w_{ij} \cdot [l_{i} \neq l_{j}], \label{eq:nodeproblem}
\end{align}  % y_{ij}^* &=& [l_{u} \neq l_{v}] 
or in the edge domain, by label each edge $e$ as cut $y_e=1$ or uncut $y_e=0$ 
\begin{align}
  y^* &= \argmin_{y \in P(G)} \sum_{ (i,j) \in E } w_{ij} \cdot y_{ij} \label{eq:edgeproblem}.%\\ 
\end{align}
As shown in~\cite{kappes_2013_arxiv} both problems are equivalent, but formulation \ref{eq:nodeproblem}
suffers from ambiguities in the formulation~\cite{kappes_2011_emmcvpr}.

% The exist an surjective mapping from a node-label $l$ to edge-labeling $y$ and
% a bijective mapping from a partitioning of $V$ to vertices of the multicut polytope $MC$.
% Consequently, 
% (i) problems \ref{eq:nodeproblem} and \ref{eq:edgeproblem} are equivalent
% and (ii) the node-labeling is not unique for a given partitioning and introduce some
% ambiguities.

% \subsubsection{Multicut Objective}

% The multicut / correlation clustering objective 
% can be formulated in different ways.



% \paragraph{Edge Indicator Variables:}
% \begin{center}
%     \begin{eqnarray}
%         y^* &=& \argmin_{y} \sum_{ e_{ij} \in E } w_{ij} \cdot y_{ij} \\
%         s.t.:& & y \in \textit{Multicut Polytope} \nonumber
%     \end{eqnarray}
% \end{center}

% \paragraph{Fully Connected Graph:}
% \begin{center}
%     \begin{eqnarray}
%         y^*   & = & \argmin_{y} \sum_{ i<j \in V } w_{ij} \cdot y_{ij} \\
%         s.t.: &  & y_{ij} + y_{jk} < y_{i,k} \quad \forall i, j, k   \nonumber
%     \end{eqnarray}
% \end{center}

% \paragraph{Node Coloring:}
% \begin{center}
%     \begin{eqnarray}
%         l^* &=& \argmin_{L} \sum_{ e_{ij} \in E } w_{ij} \cdot [l_{u} \neq l_{v}] \\
%         y_{ij}^* &=& [l_{u} \neq l_{v}]  
%     \end{eqnarray}
% \end{center}

\input{inputs/fig_notation.tex}

\section{Related Work}\label{sec:related_work}%
%
Due to the ambiguity of formulation~\ref{eq:nodeproblem},
a major branch of research has focused on solving 
relaxations of eq.~\ref{eq:edgeproblem}.
To keep the objective and system of inequalities small, they 
work on sparse graphs and
use cutting plane methods
in combination with (integer) linear programming and efficient 
separation procedures~\cite{kappes_2011_emmcvpr,andres_2011_iccv,kappes_2013_arxiv}.

With no time restrictions and integer constraints these methods can solve the problem to global optimality.
For huge problems both, separation and solving the ILP in each round, becomes very time consuming.

For planar problems, Yarkony \etal~\cite{yarkony_2012_eccv} 
suggested a column generating strategy for the outer LP-relaxation
that includes all cycle-inequalities of problem \ref{eq:edgeproblem}.
The column generation base on solving planar max cut instances.
While this method is fast, 
it only solve a relaxation of the problem and so requires additional rounding strategies,
lacks of a practical stopping condition for the column generation,
and most important  is restricted to planar problems.

An other branch of research uses move making algorithm 
to optimize correlation clustering~\cite{bansal_2004_ml,beier_2014_cvpr,Kernighan-1970}.
Starting with an initial segmentation, auxiliary problems are solved that 
strictly improve the segmentation.

Bansal and Bagon propose a modified $\alpha-$expansion~\cite{bansal_2004_ml} 
algorithm suitable for correlation clustering, that allows all variables to 
change to cluster $\alpha$ in a single move. While the authors claim that this
scales to large scale data, in~\cite{beier_2014_cvpr} it has been shown that 
this is not the case in general. This is not surprising because the auxiliary 
problems can have as many variables as the original problem. 
 
A more efficient move making method has been presented by Beier \etal~\cite{beier_2014_cvpr}.
Their Cut, Glue and Cut method iteratively re-optimizes the cuts between adjacent clusters of the current solution.
While this method scales better then  modified $\alpha-$expansion, it still faces three problems:
Firstly, the subproblems can be as large as the original problem,
secondly, the subproblems are max-cut problems on non-planar graphs that are NP-hard 
and even the used approximation has high polynomial complexity.
This limits the method, to be applicable to huge problems. The same holds for the 
method of Kernighan and Lin~\cite{Kernighan-1970}. 

   % \begin{itemize}
   % \item Multicut~\cite{kappes_2011_emmcvpr}
   % \item Expand and Explorer~\cite{bagon_2011_arxiv}
   % \item Fast Planar CC~\cite{yarkony_2012_eccv}
   % \item Break and Conquer \cite{alush_2013_simbad}.
   % \item Cut Glue And Cut~\cite{beier_2014_cvpr}
   % \end{itemize}

For energy minimization problems fusion moves have become increasingly popular~\cite{Lempitsky-2010,kappes_2014_ws}.
For many large scale computer vision applications fusion moves lead to good approximations
with state of the art any time performance~\cite{kappes_2014_ws}.

The fusion move algorithm iteratively fuse the current best solution with a proposal solutions
by optimizing over the subspace spanned by the two labeling. 
Due to the ambiguity of a node-labeling, fusion moves can not be applied directly for correlation clustering.
We will show how to overcome this point later.

\input{inputs/fig_hc_alg.tex}

Outside computer vision greedy methods has been suggested for correlation clustering problems, see \cite{Elsner-2009} for an overview.
A common greedy approach is to randomly permute the nodes and than assign 
iteratively each node to an existing cluster or create a new cluster if costs cannot be decreased by assigning to a cluster.
Common assigning strategies are:
\emph{BEST}; assign to cluster which is linked by the most positive edge~\cite{Ng-2002},
\emph{FIRST}; Assign to the cluster which is first linked by a positive edge~\cite{Soon-2001}, and
\emph{VOTE}; Assign to cluster that minimizes objective function~\cite{Elsner-2008}.
%
The PIVOT Algorithm~\cite{Ailon-2008} iterate over all nodes in random order.
If the node is not assigned it construct a cluster containing the node and all its 
unassigned positively linked neighbors.  
%
Typically these algorithms started for many random permutations, 
and pick the clustering with best objective value.

A widely use post-processing method is  Best One Element Move (BOEM)~\cite{Gionis-2007}.
Start with an initial clustering one node is removed from a cluster and reassigned to an existing or new cluster such that the costs are minimized.
BEOM stops if no move can decrease the costs.

% \subsection{Karger-Stein Algorithms}
% Use randomized procedure to reduce the number of nodes / edges to a reasonable 
% number.
% On the smaller graph, more expensive solvers are used.


%\input{inputs/fig_hc_alg.tex}

\section{Energy Based Hierarchical Clustering}\label{sec:ehc}
Agglomerative hierarchical clustering (HC) is widely used
in graph / image segmentation~\cite{arbelaez_2006}.
In each step, the edge with the highest weight $w$ is
contracted (red edges in Fig.~\ref{fig:hc_alg}).
Doing so, parallel edges edges can occur.
In agglomerative clustering, weights of
parallel edges are merged into single edges.
For image segmentation, the length
weighted mean is used to do this update~\cite{arbelaez_2006}.

While we, contrary to~\cite{arbelaez_2006}, directly work on energies, we are interesting in an 
agglomeration with respect to our energy function.
For this we propose to use energy based agglomeration
withe the following update rule:
Whenever there are multiple edges between 
a pair of nodes, these edges are merged into a single edge
by summing up their weights, since we 
minimize the sum of the cut edges.
We call HC with this update method Energy base Hierarchical Clustering (EHC).

We stop EHC if the highest edge weight is
smaller or equal to zero  (green edge in Fig.~\ref{fig:hc_alg}). Any further edge
contraction leads to worse energies.

% Hierarchical clustering is fast, and 
% due to the agglomeration of edge weights, 
% {\color{red} hierarchical clustering is robust against noise}.
% {\color{yellow} [ First most similar nodes are merged...]
% In agglomerative clustering, weights of
% parallel edges are merged into single edges.}
% For image segmentation, the length
% weighted mean is used to do this update [ref].

% [We contrary on those works work directly on energy functions and that why ...]
% We propose to use Energy Based Hierarchical Clustering (EHC) to find
% solutions for correlation clustering.


% To make EHC follow the energy, we propose 
% the following update rule:
% Whenever there are multiple edges between 
% a pair of nodes, these edges are merged into a single edge
% by summing up their weights, since we 
% minimize the sum of the cut edges.
% The algorithm stops if the highest edge weight is
% smaller or equal to zero  (green edge in fig. ~\ref{fig:hc_alg}). Any further edge
% contraction leads to worse energies.

Given the intrinsic greediness of hierarchical clustering, 
and the fact that EHC does only local decisions, it is 
sensitive against salt and paper noise 
and we do not expect very good solutions.

But EHC is very fast, and can be used to initialize
CGC~\cite{beier_2014_cvpr}. Excessive time in CGC
is spend in the \emph{cut phase} to solve the 
first 2 coloring on the complete graph.
As shown in Sec.~\ref{sec:exp},
allowing CGC to start from the EHC solution
can improve performance drastically.



% Common cluster distances
% \begin{itemize}
% \item Single-Link: Nearest Neighbor - their closest members.
% \item Complete-Link: Furthest Neighbor - their furthest members.
% \item Centroid: Distance between centroids
% \item Group Average: average of all cross-cluster pairs.
% \end{itemize}

%------------------------------------------------------------------------
\section{Correlation-Clustering-Fusion Moves}\label{sec:cc_fm}


\input{inputs/fig_fusion.tex}

\textbf{new}

Fusion moves as defined in \cite{Lempitsky-2010} work
in the node domain and do not work properly for 
objective functions as Eq.~\ref{eq:nodeproblem} since
the node coloring is ambiguous and has no semantic meaning.
In the following, we propose a more suitable fusion move for correlation
clustering which works on the edge domain.
%
Given two proposal solutions $y'$ and $y''$,
$\mathcal{C}$ is the set of edges
which are uncut in $y'$ and $y''$.
%
\begin{align}
\breve{y}    & =  y' \cup y'' \\
\mathcal{C}  & =  \{ (i, j) \quad | \quad \breve{y}_{ij} = 0 \}
\end{align}
%
The fusion move for correlation clustering is solving Eq.~\ref{eq:edgeproblem}
with additional \emph{must-link constraints} for all edges in $\mathcal{C}$.
%
\begin{align}
  y^* &=& \argmin_{y \in P(G)} \sum_{ (i,j) \in E } w_{ij} \cdot y_{ij} \label{eq:fusion_move_a}.\\ 
  s.t & &  y_{ij} = 0 \quad \forall (i, j) \in \mathcal{C} \nonumber
\end{align}
%
By construction, solving Eq.~\ref{eq:fusion_move_a} cannot increase the energy:
\begin{align}
  \sum  w_{ij} \cdot y^*_{ij}  \leq \min \left( \sum  w_{ij} \cdot y'_{ij} ,  \sum   w_{ij} \cdot y''_{ij} \right)
\end{align}
%
Eq.~\ref{eq:fusion_move_a} can be solved efficiently, since 
we can work on a coarser graph, where all nodes which are connected
via must-link constraints are merged into single nodes.
This coarser graph is constructed by contracting all must-link edges.




\textbf{old}

Motivation Using Fusion moves -> problem ambiguity

A consistent edge labeling $y$ defines a segmentation of $V$ by enumerating the connected components 
in $G|_y=(V,\{e\in E|y_e=0\}$ by $l_v = \#(v)$, where $\#(v)$ denotes the number of the connected component of the node $v\in V$.

We define the contradiction of graph $G_y=(V_y,E_y)$
with $V_y=\{\#(v)|v\in V\}$ and $E_y=\{\#(v),\#(u)|uv\in E, y_{uv}=1\})$.

%\input{inputs/fig_alg_graph.tex}

\input{inputs/fig_contraction.tex}
\input{inputs/fig_projection.tex}

\subsection{Define Fusion on Segmentations}

Given two proposal solutions $y'$ and $y''$
the fusion move between those solutions is defined
as follows:

Let the $\bar{y}$ be the edge wise or of  $y'$ and $y''$.
\begin{equation}
\breve{y} =  y' \cup y''.
\end{equation}
Contraction of all edges in $G$ where $\bar{y}=0$ defines
a smaller graph $G_{\bar{y}}$ and the corresponding
edge weights $W_{\hat{y}}$.
Any cut $\bar{y}$ on $G_{\hat{y}}$ induces a cut $y$ on $G$.
The function 
$\rho_y^{-1}$ maps a cut from $G_{\hat{y}}$  to $G$ 

\begin{equation}
   \rho_y^{-1} : P(G_y) \to P(G)
\end{equation}

Therefore solving eq. \ref{eq:edgeproblem} 
on $G_{\hat{y}}$
leads to a cut on $G$ which is guaranteed to 
to have a better or equal energy
then any of the two proposal solutions.

\input{inputs/fig_move_graph.tex}


% Global optimal solvers for multicut do not scale beyond ??? \cite{???}.
% Good approximate solvers for planar graphs exist \cite{beier_2014_cvpr,yarkony_2012_eccv} 
% but have difficulties to find good solutions for non planar graphs \cite{beier_2014_cvpr}.

% To make multicuts scalable, it is desirable to replace the graph $G$
% with a graph $\hat{G}$, which is smaller in the number of nodes and edges. 
% If $\hat{G}$ is small enough, it is feasible to use global optimal solvers.

% A trivial way to reduce the size of the graph  is edge contraction.
% Until a desirable size is reached, one might contract the edge with
% the highest weight. A cut on the contracted graph  $\hat{G}$ is always
% a valid cut on $\hat{G}$.

% On the other hand, if an edge $e$ is contracted, it will never be cut.
% If $e$ is part of the cut in a global optimal solution, we cannot find
% this cut.

% To overcome this problem  we can randomize the edge contraction,
% and repeat this multiple times and remember the best solution.

% But this would throw away all the ``not the best'' solutions (rewrite me).
% Within this work we propose the following nifty trick.
% Instead of throwing away solutions, we intersect them with the 
% best solution, and solve the multicut on the resulting
% graph.
% Therefore we fuse multiple solutions,



%\begin{algorithm}
%\begin{scriptsize}
%\caption{Fusion Based Algorithms}\label{alg:fusion}
%
%
%
%
%\begin{algorithmic}[1]
%\Procedure{Intersection-Based-Inference}{GEN,$J$,$X$}
%\State $x^0 \gets \textrm{initial state form } X$
%\State $n \gets 0 $                                 \Comment{Number of moves}
%\State $m \gets 0 $                                 \Comment{Number of moves without progress}
%\While{$m < m_{\max}$ and $n < n_{\max}$ }
%\State $n \gets n+1$ 
% \State $x'\gets GEN(x^{n-1},J,X)$                    \Comment{Generate proposal}
% \If{$J(x^{n-1}) \leq J(x')$}                       
%   \State $x^{n} \gets Fuse(x^{n-1},x',J)$             
% \Else
%   \State $x^{n} \gets Fuse(x',x^{n-1},J)$
% \EndIf
% \If{$J(x^{n}) \leq J(x^{n-1})$}
%   \State $m \gets 0$                                 \Comment{Reset counter}
% \Else
%   \State $m \gets m+1$                               \Comment{Increment counter}
% \EndIf
%\EndWhile
%\State \textbf{return} $x^n$
%\EndProcedure
%\end{algorithmic}
%\end{scriptsize}
%\end{algorithm}




%\begin{algorithm}
%\begin{scriptsize}
%\caption{Fusion Moves}\label{alg:fusion_moves}
%\scriptsize
%\begin{algorithmic}[1]
%\Require $J(x) \leq J(x')$
%\Ensure $J(\hat{x}) \leq J(x)$
%\vspace{0.2cm}
%\Procedure{Fuse}{$x,x',J$}
%\State $\bar{x} \gets \textbf{\scriptsize{Intersect}}(x,x')$  
%    \Comment{
%        \begin{minipage}{0.4\linewidth} 
%            \begin{tiny}
%            \begin{singlespace}
%                intersect
%                uncut edges in $x$ and $x'$,
%                return  
%                connected component labeling
%            \end{singlespace}
%            \end{tiny}
%        \end{minipage}
%    }
%\vspace{0.1cm}
%\State $ \tilde{G}=(\tilde{E},\tilde{V}),\tilde{W} 
%    \gets \textbf{\scriptsize{Contract}}(G,W,\bar{x})$  
%    \Comment{ 
%        \begin{minipage}{0.2\linewidth} 
%            \begin{tiny}
%            \begin{singlespace}
%                Contract all edges 
%                which are uncut in $\bar{x}$ 
%            \end{singlespace}
%            \end{tiny}
%        \end{minipage}
%    }
%\vspace{0.1cm}
%\State $\tilde{x}^*= \argmin\limits_{\tilde{x}} \sum\limits_{e_{ij} \in \tilde{E}} \tilde{w}_{ij}[\tilde{x}_i \neq \tilde{x}_j]$
%    \Comment{ 
%        \begin{minipage}{0.3\linewidth} 
%            \begin{tiny}
%            \begin{singlespace}
%                Solve the multicut objective
%                on smaller graph $\tilde{G}=(\tilde{V},\tilde{E})$
%            \end{singlespace}
%            \end{tiny}
%        \end{minipage}
%    }
%
%\State $\hat{x} \gets \textbf{\scriptsize{ProjectBack}}(\tilde{x}^*)$
%    \Comment{ 
%        \begin{minipage}[\textheight]{0.4\linewidth} 
%            \begin{tiny}
%            \begin{singlespace}
%                Translate the connected component
%                labeling $\tilde{x}^*$ of $\tilde{G}$ 
%                to a connected component labeling of $G$
%            \end{singlespace}
%            \end{tiny}
%        \end{minipage}
%    }
%\State \textbf{return} $\hat{x}$
%\EndProcedure
%\vspace{0.3cm}
%\Procedure{FUSE$_{\textrm{BASE}}$}{$x,x',J$}
%\State \textbf{return} ${\arg\min}_{\bar{x} \in \{x,x'\}}J(\bar{x})$
%\EndProcedure
%\end{algorithmic}
%\end{scriptsize}
%\end{algorithm}






%-------------------------------------------------------------------------
\subsection{Fusion Move Solver}




\begin{center}
    \begin{eqnarray}
        y 
        &=& 
        \rho_{\hat{y}}^{-1}(
        \argmin_{y \in MC(G_{\hat{y}})\cap \{0,1\}^{|E_{\hat{y}}|}} 
        \sum_{ (i,j) \in E_{\hat{y}} } 
        w^{\hat{y}}_{ij} \cdot \bar{y}_{ij} 
    \end{eqnarray}
\end{center}

\begin{center}
    \begin{eqnarray}
      y \cdot w \leq \min(y'\cdot w,  y'' \cdot w) 
    \end{eqnarray}
\end{center}


%-------------------------------------------------------------------------
\subsection{Proposal Generators}

\noindent \textbf{Desired Properties:}
As discussed in \cite{Lempitsky-2010}, proposals
should have two properties: \emph{quality} 
and \emph{diversity}.
The quality / energy of the individual proposals
ensures low energy solutions since the
result of the fusion cannot be higher then
the energy of the current best solution.
Diversity between the individual proposals
reduces the chances to get stuck in local minima.
For correlation clustering fusion we add a third
property: \emph{size}.
The size of the contraction graph directly
depends on the number of connected components
in the proposal solution and the current best solution.
The extreme  case is a proposal where each 
node is in a separate connnected component.
Solving the fusion move with such a proposal
is equivalent to solving the original problem.
Solving the fusion move with a proposal with 
a single connected component will result 
in the current best solution.
Therefore the size of the proposals
should be small enough, such that solving
eq.~\ref{eq:edgeproblem} can
be done fast enough, but large enough to make
meaningful moves (??? this last sentence needs to be rewritten).




%\begin{algorithm}
%    \begin{scriptsize}
%        \caption{Proposal Generators}\label{alg:proposal_gen}   
%        \begin{algorithmic}[1]
%            %\Require $J(x) \leq J(x')$
%            %\Ensure $J(\hat{x}) \leq J(x)$
%            %\vspace{0.3cm}
%            \Procedure{Rand2Coloring}{$x,G=(E,V),W$}
%            \State $\tilde{W} \gets \textbf{\scriptsize{randomize}}(W) $
%            \State $x'= \argmin\limits_{\bar{x}} \sum\limits_{e_{ij} \in E } \tilde{w}_{ij}[\bar{x}_i \neq \bar{x}_j]  [x_i = x_j]   $
%
%            $ \quad\quad s.t.\quad x_i \in \{0, 1\} \forall i $
%            \State \textbf{return} $x'$
%            \EndProcedure
%            %
%            %
%            \vspace{0.3cm}
%            \Procedure{RandEdgeWeightedWatersheds}{$x,G=(E,V),W$}
%            \State $\tilde{W} \gets \textbf{\scriptsize{randomize}}(W) $
%            \State $ getRandomSeeds$
%            \State $ runEdgeWeightedWs$
%            \EndProcedure   
%            %
%            %
%            \vspace{0.3cm}
%            \Procedure{RandHierarchicalClustering}{$x,G=(E,V),W$}
%            \State $\tilde{W} \gets \textbf{\scriptsize{randomize}}(W) $
%            \State $\hat{G}=(\hat{E},\hat{V}) \gets G=(E,V)$
%            \State $\hat{W} \gets W$
%            \While{$|\hat{V}|>\gamma$ \textbf{and}  $\max(\hat{W})>\theta$ }
%                \State $\hat{G}=(\hat{E},\hat{V}),\hat{W} \gets 
%                    \textbf{\scriptsize{contract}}(\hat{G},\hat{W},  \argmax\limits_{\hat{E}}(\hat{W}) )$
%            \EndWhile
%            \State $x' \gets \textbf{\scriptsize{getClusterResults}}(???) $ 
%            \EndProcedure
%        \end{algorithmic}
%    \end{scriptsize}
%\end{algorithm}



\subsubsection{Randomized HierarchicalClustering (RHC)}

To generate proposals cut we can use bottom up hierarchical clustering.
In each step we contract the edge with the highest weight.
Parallel edges are merged into single edges by summing their weights.
We stop when a certain number of nodes is reached, or the
largest edge weight among the not yet contracted edges is smaller then a certain threshold.
To get versatile proposals we either add noise 
to the edge weighs or permute a certain number of weights.


%\subsubsection{Randomized MaxCut  (RMC)}
%
%To get energy aware proposals we can use max cuts.
%The max cut objective is the same as
%the multicut objective, but only two 
%node colors are allowed.
%Therefore solutions of the max cut objective
%might be useful proposals.
%To create different proposals we set 
%the weight of edges which are cut 
%in the best solution to zero.
%In this way we favor solutions 
%different form the current best. 
%In addition we either add noise permute a certain number of weights.


\subsubsection{Randomized Watersheds (RWS)}

The edge weighted watershed algorithm \cite{meyer_2013}
with random seeds can be used to get 
cheap proposals. Instead of $n$ seeds distributed uniformly
over all nodes we use the following.
Draw $n/k$ random edges only the negative edges, 
and give the two nodes of each random edge different seeds.
Doing so, a random subset of negative edges is forced
to be cut within each proposal.
For additional randomness, noise can be added to
the edge weighs.



%\input{inputs/fig_hc.tex}

\subsection{Properties and Polyhedral Interpretation}
An alternative interpretation of our method is shown in Fig.~\ref{fig:polyheadral}.
In each iteration the current and proposed segmentation define an inner polyheadral 
approximation of the original multicut polytope. 
Optimizing over this polytope is the same problem as the original one, but much smaller.
Furthermore, the costs does not change and and improvement in the smaller polytope will 
be the same in the original graph.
The choice of the proposal defines the shape of the auxiliary polytope. 
In the given toy example, the first (red) polytope give a huge improvement, the second proposal
defines the blue polytope which does not lead to an improvement. 
The third proposal generates the green polytope that includes the global optimal solution.

This procedure is fundamental different to common polyhedral multicut methods~\cite{kappes_2011_emmcvpr,kappes_2013_arxiv}, 
which tighten a outer relaxation of the multicut polytope and contrary to our method do not operate in the feasible domain. 



\begin{figure}
\centering
\input{polytope-tikz.tex}
\caption{Each move can be interpreted as an optimization of an inner polyhedral optimization.
Each polyhedral approximation includes the current vertex. Starting with $y_0$ we optimize over the red polytope 
and find $y_1$ as optima. When optimize over the blue polytope we stay in $y_1$ as optima.
When optimizing over the green polytope we find $y_2$ which we will never leaf again or any polytope containing $y_2$. 
}
\label{fig:polyheadral}
\end{figure}


\begin{theorem}[Equivalence]
The optimal multicut $\bar{y}^*$ on the contracted graph $G_y$
is equivalent to the optimal multicut $y^*$ in the original graph $G$ 
with additional must link constraints ($y^* \leq y$).
%
\begin{proof}
Since $y^*$ fulfills all  must link constraints, we have  $y^*\cdot w = \rho^{-1}_y(y^*)\cdot \bar{w}$.
Consequently, $y^*\cdot w = \rho_y(\bar{y}) \cdot w$ and  $\rho^{-1}_y(y^*)\cdot \bar{w}= \bar{y}\cdot\bar{w}$.
\end{proof}
%
\end{theorem}




\begin{theorem}[Monotone]
Let $y'$ and $y''$ be two proposal solutions and  $y^*$ the optimal fused segmentation.
Than $y^*$  will never decrease the energy:
$y^* \cdot w \leq  \min( y'\cdot w,  y''\cdot w) $ 
%
\begin{proof}
The feasible set of the fusion move includes per definition  $y'$ and $y''$.
If $y'$ or $y''$ would have lower multicut cost than $y^*$, $y^*$ would not be optimal 
and that why not the propose segmentation of a fusion.
\end{proof}
%
\end{theorem}


\begin{remark}[Fix-point (pointless to me)]
Let $Y$ be the set of possible proposals, than
$y'$ is a fix-point if $y'=\arg\max_{y\in P(G_{y'\OR y''})} y\cdot w$ for all $y''\in Y$
\end{remark}



\begin{remark}[Optimality]
Let $y'$ be the current and $y''$ the proposed segmentation.
If the optimal solution $y^*$ lies in $P(G_{y' \OR y''})$ than a optimal fusion move will return $y^*$.
\end{remark}



\section{Experiments}\label{sec:exp}

Within all experiments we used the following existing solvers 
with public available implementation.
\begin{itemize}
    \item 
    \textbf{CGC} \cite{beier_2014_cvpr} we used the public available 
    implementation\footnote{\label{opengm-cvpr2014}\url{https://github.com/opengm/opengm/tree/cgc-cvpr2014}}.
    As reported in \cite{beier_2014_cvpr}, CGC is faster and gives better energies then \cite{yarkony_2012_eccv} and \cite{bagon_2011_arxiv}.
    Therefore we exclude those in our experiments.
    \item \textbf{KL}\cite{???}  and a public available 
        implementation 
        \footlabel{opengm-master}{%
        %\footnote{
        %    \label{opengm-master} 
            \url{https://github.com/opengm/opengm}
            %\label{opengm-master}
        }
    \item \textbf{MC-R,MC-I}\cite{kappes_2011_emmcvpr}  where we used a  modified 

    version of a public available implementation implementation \footref{opengm-master}.

    We replace a highly used queue with a more suitable and
    fast \emph{index-min-heap}\cite{???}.
    MC-R indicates that only the LP is solved, while MC-I solves
    the ILP.
    \item non vision solvers here TODO
\end{itemize}
And the following proposed solvers:
\begin{itemize}
\item \textbf{EHC}: Energy Based Based Hierarchical Clustering 
as described in sec.~\ref{sec:ehc}.
\item \textbf{EHC-CGC}: As CGC but warm started 
with the solution from EHC.
\end{itemize}

 \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \begin{semilogxaxis}[  mark size=1pt,
   % restrict y to domain=-6000:-4400,
   %   restrict x to domain=10:4000,
    xlabel = {runtime},
    xmin = 0,
    xmax = 4100,
    legend to name = ledgendPositionMC,
    legend columns=4,
    width = 1.0\columnwidth,
    scaled ticks = false
    ] 
    \addplot[color=red,mark=x] table[x=time, y=MC-CCFDB-o*]{data3/knott-3d-450b.data};          \addlegendentry{MCR-old}
    \addplot[color=orange,mark=o] table[x=time, y=MC-CCFDB-n*]{data3/knott-3d-450b.data};          \addlegendentry{MCR-new}
    \addplot[color=blue,mark=x] table[x=time, y=MC-CCIFD-o*]{data3/knott-3d-450b.data};          \addlegendentry{MCI-old}
    \addplot[color=cyan,mark=o] table[x=time, y=MC-CCIFD-n*]{data3/knott-3d-450b.data};          \addlegendentry{MCI-new}

    \end{semilogxaxis}
    \end{tikzpicture}
  \begin{center}
  \hypersetup{linkcolor = black}
  \ref{ledgendPositionMC}
  \hypersetup{linkcolor = red}
  \end{center}
    \caption{knott-3d-450b}\label{fig:at:knott-450b}
  \end{figure}



\subsection{Parameter Choice}
\begin{figure}
\centering
\fbox{\includegraphics[width=0.45\columnwidth,trim=3.5cm 8cm 4.5cm 8cm,clip]{images/grid-time2.pdf}}
\fbox{\includegraphics[width=0.45\columnwidth,trim=3.5cm 8cm 4.5cm 8cm,clip]{images/grid-value2.pdf}}
\caption{Empirical evaluation of the impact of noise used for proposal generation and proposal size.
  Proposals with many segments causes longer runtime. Noise seemed not to be a critical parameter but should be selected large enough.
}
\end{figure}

\subsection{Synthetic Models}

\subsection{Social Networks}
A application for large scale correlation clustering are social networks.
We consider two of those networks from the Stanford Large Network Dataset Collection\footnote{\url{http://snap.stanford.edu/data/index.html}}.
Both networks are given by weighted directed graphs with edge weights $-1$ and $1$. 
%
The first network is called \emph{Epinions}. 
This is who-trust-whom online social network of a a general consumer review site Epinions.com. 
Each directed edge $a\to b$ indicated that user $a$ trusts  or does not trust user $b$ is the edge-weight is positive or negative, respectively.
The network contains $131828$ nodes and $841372$ edges from which $85.3\%$ are positively weighted.
%
The first network is called \emph{Slashdot}. 
Slashdot is a technology-related news website know for its specific user community. 
In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. 
The network was obtained in November 2008 and contains $77350$ nodes and $516575$ edges from which $76.73\%$ are positively weighted.

We consider the problem to cluster this graphs such that positively weighted edges link inside and negatively weighted between clusters.
In other words friends and people who trust each other should be in the same segment and foes and non-trusting people in different clusters.
% 
To compensate the high impact of nodes with high degree we can normalize the edge weights such that each person has the same impact to the overall network, by enforcing.
\begin{align}
  \sum_{i\to j \in E_{\to}} |w_{i\to j}| &= 1&\forall i\in V, deg^{\textrm{out}}(i)\geq 1 
\end{align}
We define the following energy function
\begin{align}
 J(y) &= \sum_{i\to j \in E^+_{\to}} y_{ij}\cdot w_{i \to j} +  \sum_{i\to j \in E^-_{\to}} (y_{ij}-1)\cdot w_{i \to j} \nonumber\\
      &= \sum_{ij \in E} y_{ij}\cdot \underbrace{(w_{i \to j}+w_{j \to i})}_{w_{ij}} + \textrm{const}
\end{align}
which is zero if the given partitioning does not violated any relation and larger otherwise.


\subsection{Real World Models}
To segment images or volumes into a previously
unknown number of clusters, correlation clustering
can be used.
Starting from a super-pixel / super-voxel segmentation
which defines a region adjacency graphs (RAG),
correlation clustering finds the cut with the lowest energy.
The energy is based on a likelihood of merging adjacent super-voxels.
Each edge has a probability to keep adjacent segments separate ($p(y_{ij} =1)$)
or to merge them ($p(y_{ij} = 0)$).
The energy function is defined as following:
\begin{align}
 J(y)  &= \sum_{ij \in E} y_{ij}\cdot \underbrace{  log\left( \frac{p(y_{ij} =0)}{p(y_{ij} =1)}\right) + log \frac{1-\beta}{\beta}  }_{w_{ij}}
\end{align}
Where $\beta$ is used as a prior.

For 2D images, we took the models from \cite{andres_2011_iccv}
which are defined on the images of the Berkeley Segmentation
Database \cite{martin_2001}.
For 3D volume segmentation we took the models from
Andres \etal \cite{kroeger_2012_eccv}.
We use models with the cube sizes $150^3$, $300^3$, $450^3$, $550^3$ and $900^3$.
Both, the 2D and 3D models are available in a recent graphical model
benchmark of Kappes \etal ~\cite{kappes_2013_benchmark_cvpr}.
We use the same names for the datasets as in the benchmark,
\emph{image-seg} for the models from  \cite{andres_2011_iccv} and \emph{knott-150 - knott-550} and \emph{3d-seg}
for the models from \cite{kroeger_2012_eccv}.
The size of instances is given in tab. ~\ref{tab:instance_sizes}.


\begin{table}[H]
   \tiny
   \centering
   \caption{Datasets used for evaluation}
   \label{tab:instance_sizes}
   \begin{tabular}{llllc}
      \toprule
         Dataset          &     \#Nodes     & \#Edges  & \#Instances  & Planar        \\ 
      \midrule 
         image-seg        &     156-3764        & ???        & 100          & $\bullet$    \\ 
         knott-150        &     572-972         & ???        & 2            & $\times$ \\
         knott-300        &     3846 - 5896     & ???        & 2            & $\times$ \\
         knott-450        &     15150-17070     & ???        & 2            & $\times$ \\
         knott-550        &     27286 - 31249   & ???        & 2            & $\times$ \\
         3d-seg           &     7958 - 101220   & ???        & 2            & $\times$ \\
         slashdot         &     ???             & ???        & ???          & $\times$ \\     
      \bottomrule
   \end{tabular}
\end{table}

%\begin{center}
%    \begin{figure}
%        \begin{center}
%            \begin{subfigure}[b]{0.45\linewidth}
%                \includegraphics[width=1.0\linewidth]{images/-010.jpg}
%            \end{subfigure}
%            \quad
%            \begin{subfigure}[b]{0.45\linewidth}
%                \includegraphics[width=1.0\linewidth]{images/-030.jpg}
%            \end{subfigure}
%        \end{center}
%    \caption{
%        Raw FIBSEM data and correlation clustering result of the model from \cite{kroeger_2012_eccv%}
%    }
%    \end{figure}
%\end{center}
%
%\newpage



\input{anytimefigure2.tex}


\begin{figure}
\input{scatterplots/knott-3d-450.tex}
\caption{Values and runtime for the 8 knott-3d-450 instances. Note that we use for DYNCC a defensive stopping condition.
A more aggressive one would lead to much faster runtime with only a bit worse values.
DYNCC find solutions with less than distance 100 to the optimum a magnitude faster than the state of the art, which produce poor results on hard instances with a 1 hour time limit.
}
\end{figure}



%\subsection{Pixel-wise Multicuts}
%\input{inputs/fig_pixel_mc.tex}


\section{Further Work}\label{sec:future}
So far we have generated the proposals randomly and do not take the current segmentation into account.

So far we only enforce must link constraints to obtain smaller problems.
One can also enforce cannot link constraints to the problem. 
If those are closed, they automatically lead to a decomposition of the problem, 
which is a interesting property for very huge data.

Improving proposals.




\section{Conclusion}\label{sec:conclusion}

We have presented a fast and scalable 
approximate solver for correlation 
clustering, named Fusion Moves for correlation clustering (FM-CC).
The best solution is iteratively improved 
by a fusion with proposal solutions.
The itself is formulated as correlation
clustering on a smaller graph fewer edges and nodes
and can therefore be solved very fast.
Our evaluation shows that CC-FF
significantly outperforms 
other solvers w.r.t. any time performance 
with increasing problem size.


    


\newpage

\FloatBarrier
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\newpage

\onecolumn
\section{Appendix}

\input{tables/anytime/image-seg.tex}
\input{tables/anytime/knott-3d-150.tex}
\input{tables/knott-3d-300.tex}
\input{tables/anytime/knott-3d-450.tex}
\input{tables/knott-3d-550.tex}
\input{tables/anytime/seg-3d.tex}
\input{tables/modularity-clustering.tex}



\end{document}
